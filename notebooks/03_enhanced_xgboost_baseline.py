"""
Phase 3 Enhanced: XGBoost Baseline with Clean High-Quality Data
XGBoost Explainable AI for Patient Treatment Cost Prediction

Author: Ammar Pavel Zamora Siregar (1202224044)
Date: September 2024
Objective: Establish XGBoost baseline with clean data (Target RÂ² > 0.87)

This script implements Algorithm 3 from the thesis methodology using the enhanced
preprocessed data with 10.00/10.0 quality score. Expected significant improvement
over previous attempts due to:

1. Clean data with corrected feature engineering (high_risk r=0.815)
2. Strategic interactions (smoker_bmi_interaction r=0.845)
3. Enhanced baseline comparison (Linear RÂ² = 0.8566)
4. Medical domain expertise integrated into features

With Enhanced Linear Regression achieving RÂ² = 0.8566 (gap: 0.0134 to target),
XGBoost should easily achieve the thesis target RÂ² > 0.87.

Target: Demonstrate XGBoost superiority over enhanced baseline and achieve thesis target
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import LabelEncoder
import warnings
import os
import json
import pickle
from datetime import datetime
import time
warnings.filterwarnings('ignore')

# Configure plotting
plt.style.use('default')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (14, 10)
plt.rcParams['font.size'] = 12

# Set random seed for reproducibility
np.random.seed(42)

class EnhancedXGBoostBaseline:
    """
    Enhanced XGBoost baseline using high-quality preprocessed data.
    Expected to achieve RÂ² > 0.87 due to improved data quality and feature engineering.
    """

    def __init__(self):
        self.model = None
        self.feature_names = None
        self.performance_metrics = {}
        self.baseline_comparison = None
        self.preprocessing_quality = None

    def load_enhanced_data_and_baseline(self):
        """Load enhanced data and baseline results for comparison."""
        print("=" * 70)
        print("ENHANCED XGBOOST BASELINE - ALGORITHM 3")
        print("=" * 70)
        print("Loading enhanced preprocessed data and baseline results...")

        # Load enhanced processed data
        try:
            df = pd.read_csv('data/processed/insurance_processed.csv')
            print(f"âœ… Enhanced dataset loaded: {df.shape[0]} records, {df.shape[1]} features")
        except FileNotFoundError:
            print("âŒ Enhanced processed data not found!")
            print("Please run: python notebooks/00_enhanced_data_preprocessing.py")
            return None, None

        # Load preprocessing quality information
        try:
            with open('data/processed/preprocessing_enhanced_summary.json', 'r') as f:
                self.preprocessing_quality = json.load(f)
                quality_score = self.preprocessing_quality.get('quality_score', 0)
                print(f"âœ… Data quality score: {quality_score}/10.0")
        except FileNotFoundError:
            print("âš ï¸  Quality summary not found")

        # Load enhanced baseline results for comparison
        try:
            with open('results/models/enhanced_baseline_summary.json', 'r') as f:
                self.baseline_comparison = json.load(f)
                baseline_r2 = self.baseline_comparison['performance_metrics']['test']['r2_score']
                print(f"âœ… Enhanced Linear Regression RÂ² = {baseline_r2:.4f}")
        except FileNotFoundError:
            print("âš ï¸  Enhanced baseline results not found - run enhanced linear regression first!")
            print("   Proceeding without baseline comparison...")

        # Display enhanced feature correlations
        print(f"\nğŸ“Š Key Enhanced Features Performance:")
        key_features = ['high_risk', 'smoker_bmi_interaction', 'smoker_age_interaction', 'cost_complexity_score']
        for feature in key_features:
            if feature in df.columns:
                corr = df[feature].corr(df['charges'])
                print(f"   {feature}: r={corr:.3f}")

        print(f"\nğŸ¯ XGBoost Advantage Expectations:")
        print(f"   Enhanced Linear Regression: RÂ² = {baseline_r2:.4f}" if self.baseline_comparison else "   Enhanced Linear Regression: Not available")
        print(f"   Gap to thesis target (0.87): {0.87 - baseline_r2:.4f}" if self.baseline_comparison else "   Target: RÂ² > 0.87")
        print(f"   XGBoost should: Capture non-linear patterns in enhanced features")
        print(f"   Expected performance: RÂ² > 0.87 (thesis target achievement)")

        return df, self.baseline_comparison

    def prepare_features_for_xgboost(self, df):
        """
        Prepare enhanced features optimally for XGBoost.
        XGBoost handles mixed data types well, but we'll use label encoding for efficiency.
        """
        print("\n" + "=" * 50)
        print("ENHANCED FEATURE PREPARATION FOR XGBOOST")
        print("=" * 50)

        df_xgb = df.copy()

        # Label encoding for XGBoost efficiency (handles categorical features natively)
        print("ğŸ”§ Optimizing features for XGBoost:")
        label_encoders = {}
        categorical_features = ['sex', 'smoker', 'region', 'bmi_category', 'age_group']

        for feature in categorical_features:
            if feature in df_xgb.columns:
                le = LabelEncoder()
                df_xgb[feature] = le.fit_transform(df_xgb[feature])
                label_encoders[feature] = le
                print(f"   âœ… {feature}: {list(le.classes_)}")

        # Select features for modeling
        exclude_features = ['charges', 'log_charges']
        feature_columns = [col for col in df_xgb.columns if col not in exclude_features]

        X = df_xgb[feature_columns]
        y = df_xgb['charges']  # Use log-transformed charges for interpretability

        # Validate data quality for XGBoost
        print(f"\nâœ… XGBoost Data Quality Check:")
        print(f"   Missing values: {X.isnull().sum().sum()}")
        print(f"   Infinite values: {np.isinf(X.values).sum()}")
        print(f"   Data types: {X.dtypes.value_counts().to_dict()}")

        if X.isnull().sum().sum() > 0:
            print("   âš ï¸  Filling missing values with 0")
            X = X.fillna(0)

        if np.isinf(X.values).any():
            print("   âš ï¸  Replacing infinite values")
            X = X.replace([np.inf, -np.inf], [1e10, -1e10])

        self.feature_names = feature_columns

        print(f"\nğŸ“Š Enhanced XGBoost Feature Set:")
        print(f"   Total features: {len(feature_columns)}")
        print(f"   Original features: 6")
        print(f"   Enhanced features: {len(feature_columns) - 6}")
        print(f"   High-value features: high_risk, smoker_bmi_interaction, smoker_age_interaction")
        print(f"   Target: charges (${y.min():,.0f} - ${y.max():,.0f})")

        return X, y, label_encoders

    def split_data_consistent(self, X, y):
        """Split data using consistent strategy for fair comparison with baseline."""
        print("\n" + "=" * 50)
        print("DATA SPLITTING (CONSISTENT WITH ENHANCED BASELINE)")
        print("=" * 50)

        # Use same splitting strategy as enhanced baseline for fair comparison
        # First split: 70% train, 30% temp
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y, test_size=0.3, random_state=42,
            stratify=pd.qcut(y, q=5, duplicates='drop')
        )

        # Second split: 15% validation, 15% test from the 30%
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42,
            stratify=pd.qcut(y_temp, q=3, duplicates='drop')
        )

        print(f"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
        print(f"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)")
        print(f"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)")

        # Verify target distribution consistency
        print(f"\nğŸ“Š Target Distribution Consistency:")
        print(f"   Train mean: ${y_train.mean():,.0f}, std: ${y_train.std():,.0f}")
        print(f"   Val mean: ${y_val.mean():,.0f}, std: ${y_val.std():,.0f}")
        print(f"   Test mean: ${y_test.mean():,.0f}, std: ${y_test.std():,.0f}")

        return X_train, X_val, X_test, y_train, y_val, y_test

    def train_enhanced_xgboost_baseline(self, X_train, y_train, X_val, y_val):
        """
        Train XGBoost baseline with reasonable default parameters.
        Expected to achieve RÂ² > 0.87 with enhanced features.
        """
        print("\n" + "=" * 50)
        print("ENHANCED XGBOOST BASELINE TRAINING")
        print("=" * 50)

        # Enhanced baseline parameters - conservative but effective
        baseline_params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'tree_method': 'hist',
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 1,

            # Conservative baseline parameters for stable performance
            'n_estimators': 200,        # Moderate number of trees
            'max_depth': 8,            # Reasonable depth for healthcare data
            'learning_rate': 0.1,      # Standard learning rate
            'subsample': 0.8,          # Light regularization
            'colsample_bytree': 0.8,   # Feature sampling
            'reg_alpha': 0,            # No L1 regularization initially
            'reg_lambda': 1,           # Light L2 regularization
            'min_child_weight': 1,     # Prevent overfitting
            'gamma': 0,                # No minimum split loss initially
        }

        print(f"ğŸš€ Training Enhanced XGBoost Baseline:")
        print(f"   Parameters: Conservative baseline optimized for healthcare data")
        for param, value in baseline_params.items():
            if param not in ['objective', 'eval_metric', 'tree_method', 'random_state', 'n_jobs', 'verbosity']:
                print(f"   {param}: {value}")

        # Create XGBoost regressor
        self.model = XGBRegressor(**baseline_params)

        # Training with evaluation monitoring
        print(f"\nğŸ“Š Training with evaluation monitoring...")
        training_start = time.time()

        self.model.fit(
            X_train, y_train,
            eval_set=[(X_train, y_train), (X_val, y_val)],
            verbose=False  # Reduce training output
        )

        training_time = time.time() - training_start
        print(f"âœ… Training completed in {training_time:.2f} seconds")

        # Cross-validation for robustness assessment
        print(f"\nğŸ”„ Cross-validation assessment...")
        cv_start = time.time()
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5, scoring='r2')
        cv_time = time.time() - cv_start

        print(f"âœ… 5-Fold CV RÂ² Score: {cv_scores.mean():.4f} (Â±{cv_scores.std() * 2:.4f})")
        print(f"   CV assessment completed in {cv_time:.2f} seconds")

        return training_time

    def evaluate_enhanced_performance(self, X_train, y_train, X_val, y_val, X_test, y_test):
        """Comprehensive evaluation of enhanced XGBoost performance."""
        print("\n" + "=" * 50)
        print("ENHANCED XGBOOST PERFORMANCE EVALUATION")
        print("=" * 50)

        # Make predictions on all sets
        y_train_pred = self.model.predict(X_train)
        y_val_pred = self.model.predict(X_val)
        y_test_pred = self.model.predict(X_test)

        # Calculate comprehensive metrics
        train_metrics = self._calculate_metrics(y_train, y_train_pred, "Enhanced XGBoost Training")
        val_metrics = self._calculate_metrics(y_val, y_val_pred, "Enhanced XGBoost Validation")
        test_metrics = self._calculate_metrics(y_test, y_test_pred, "Enhanced XGBoost Test")

        # Store performance metrics
        self.performance_metrics = {
            'training': train_metrics,
            'validation': val_metrics,
            'test': test_metrics
        }

        # Overfitting analysis
        train_test_gap = train_metrics['r2_score'] - test_metrics['r2_score']
        print(f"\nğŸ” Enhanced Model Generalization Analysis:")
        print(f"   Training RÂ²: {train_metrics['r2_score']:.4f}")
        print(f"   Test RÂ²: {test_metrics['r2_score']:.4f}")
        print(f"   Overfitting gap: {train_test_gap:.4f}")

        if train_test_gap < 0.05:
            print(f"   âœ… Excellent generalization (gap < 0.05)")
        elif train_test_gap < 0.10:
            print(f"   âœ… Good generalization (gap < 0.10)")
        else:
            print(f"   âš ï¸  Some overfitting detected (gap > 0.10)")

        return test_metrics

    def _calculate_metrics(self, y_true, y_pred, set_name):
        """Calculate comprehensive performance metrics."""
        r2 = r2_score(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

        metrics = {
            'r2_score': r2,
            'rmse': rmse,
            'mae': mae,
            'mape': mape
        }

        print(f"\n{set_name} Performance:")
        print(f"  RÂ² Score: {r2:.4f}")
        print(f"  RMSE: ${rmse:,.2f}")
        print(f"  MAE: ${mae:,.2f}")
        print(f"  MAPE: {mape:.2f}%")

        return metrics

    def compare_with_enhanced_baseline(self):
        """Compare XGBoost performance with enhanced Linear Regression baseline."""
        print("\n" + "=" * 60)
        print("ENHANCED MODEL COMPARISON: XGBOOST vs LINEAR REGRESSION")
        print("=" * 60)

        if self.baseline_comparison is None:
            print("âŒ Enhanced baseline results not available for comparison")
            return

        # Extract comparison metrics
        linear_r2 = self.baseline_comparison['performance_metrics']['test']['r2_score']
        linear_rmse = self.baseline_comparison['performance_metrics']['test']['rmse']
        linear_mae = self.baseline_comparison['performance_metrics']['test']['mae']

        xgb_r2 = self.performance_metrics['test']['r2_score']
        xgb_rmse = self.performance_metrics['test']['rmse']
        xgb_mae = self.performance_metrics['test']['mae']

        # Calculate improvements
        r2_improvement = xgb_r2 - linear_r2
        rmse_improvement = ((linear_rmse - xgb_rmse) / linear_rmse) * 100
        mae_improvement = ((linear_mae - xgb_mae) / linear_mae) * 100

        print(f"ğŸ“Š Enhanced Model Performance Comparison:")
        print(f"{'Model':<25} {'RÂ² Score':<12} {'RMSE ($)':<12} {'MAE ($)':<12}")
        print("-" * 65)
        print(f"{'Linear Regression':<25} {linear_r2:<12.4f} {linear_rmse:<12,.0f} {linear_mae:<12,.0f}")
        print(f"{'XGBoost Baseline':<25} {xgb_r2:<12.4f} {xgb_rmse:<12,.0f} {xgb_mae:<12,.0f}")
        print(f"{'Improvement':<25} {r2_improvement:<12.4f} {rmse_improvement:<12.1f}% {mae_improvement:<12.1f}%")

        print(f"\nğŸ¯ Thesis Target Achievement Analysis:")

        # Thesis target (RÂ² > 0.87)
        if xgb_r2 > 0.87:
            print(f"   ğŸ‰ THESIS TARGET ACHIEVED: RÂ² = {xgb_r2:.4f} > 0.87")
            print(f"   ğŸ† XGBoost successfully exceeds thesis requirement!")
            print(f"   ğŸš€ Ready for advanced optimization and XAI implementation!")
        else:
            gap_to_thesis = 0.87 - xgb_r2
            print(f"   ğŸ“Š Current Performance: RÂ² = {xgb_r2:.4f}")
            print(f"   ğŸ¯ Gap to thesis target: {gap_to_thesis:.4f}")
            if gap_to_thesis < 0.01:
                print(f"   âœ… Very close to target - hyperparameter optimization should achieve it!")
            else:
                print(f"   ğŸ“ˆ Hyperparameter optimization needed to achieve thesis target")

        # XGBoost vs Linear comparison
        if r2_improvement > 0:
            print(f"\nâœ… XGBoost SUPERIORITY DEMONSTRATED:")
            print(f"   ğŸ“ˆ RÂ² improvement: +{r2_improvement:.4f}")
            print(f"   ğŸ“‰ RMSE improvement: {rmse_improvement:+.1f}%")
            print(f"   ğŸ“‰ MAE improvement: {mae_improvement:+.1f}%")

            if r2_improvement >= 0.01:
                print(f"   ğŸ† Practically significant improvement (â‰¥0.01)")
            else:
                print(f"   ğŸ“Š Moderate improvement - optimization should enhance further")
        else:
            print(f"\nâš ï¸  XGBoost baseline performance:")
            print(f"   ğŸ“Š RÂ² difference: {r2_improvement:.4f}")
            print(f"   ğŸ”§ Hyperparameter optimization strongly recommended")

    def analyze_enhanced_feature_importance(self):
        """Analyze feature importance in enhanced XGBoost model."""
        print("\n" + "=" * 50)
        print("ENHANCED XGBOOST FEATURE IMPORTANCE")
        print("=" * 50)

        # Get feature importance using multiple methods
        importance_types = ['weight', 'gain', 'cover']
        importance_data = {}

        for imp_type in importance_types:
            importance = self.model.get_booster().get_score(importance_type=imp_type)
            importance_data[imp_type] = importance

        # Create comprehensive feature importance DataFrame
        feature_importance_df = pd.DataFrame(index=self.feature_names)
        for imp_type in importance_types:
            importance_values = [importance_data[imp_type].get(feature, 0) for feature in self.feature_names]
            feature_importance_df[imp_type] = importance_values

        # Sort by gain (most informative for interpretation)
        feature_importance_df = feature_importance_df.sort_values('gain', ascending=False)

        print("Top 12 Most Important Features (by gain):")
        print(feature_importance_df.head(12))

        # Analyze enhanced vs original features
        print(f"\nğŸ“Š Enhanced Feature Impact Analysis:")
        original_features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']
        enhanced_features = [f for f in self.feature_names if f not in original_features]

        original_in_top_10 = len([f for f in feature_importance_df.head(10).index if f in original_features])
        enhanced_in_top_10 = len([f for f in feature_importance_df.head(10).index if f in enhanced_features])

        print(f"   Original features in top 10: {original_in_top_10}")
        print(f"   Enhanced features in top 10: {enhanced_in_top_10}")
        print(f"   Enhancement impact: {enhanced_in_top_10/10*100:.1f}% of top features")

        # Highlight key enhanced features
        key_enhanced = ['high_risk', 'smoker_bmi_interaction', 'smoker_age_interaction', 'cost_complexity_score']
        print(f"\nğŸ¯ Key Enhanced Features Performance:")
        for feature in key_enhanced:
            if feature in feature_importance_df.index:
                rank = feature_importance_df.index.get_loc(feature) + 1
                gain = feature_importance_df.loc[feature, 'gain']
                print(f"   {feature}: Rank {rank}, Gain {gain:.0f}")

        return feature_importance_df

    def create_enhanced_visualizations(self, X_test, y_test, feature_importance_df):
        """Create comprehensive visualizations of enhanced XGBoost performance."""
        print("\n" + "=" * 50)
        print("CREATING ENHANCED XGBOOST VISUALIZATIONS")
        print("=" * 50)

        y_test_pred = self.model.predict(X_test)
        residuals = y_test - y_test_pred

        fig, axes = plt.subplots(2, 3, figsize=(20, 14))

        # 1. Model Performance Comparison
        if self.baseline_comparison:
            models = ['Enhanced\nLinear Regression', 'Enhanced\nXGBoost Baseline']
            r2_scores = [
                self.baseline_comparison['performance_metrics']['test']['r2_score'],
                self.performance_metrics['test']['r2_score']
            ]
            colors = ['skyblue', 'darkgreen']
        else:
            models = ['Enhanced\nXGBoost Baseline']
            r2_scores = [self.performance_metrics['test']['r2_score']]
            colors = ['darkgreen']

        bars = axes[0, 0].bar(models, r2_scores, color=colors, alpha=0.7)
        axes[0, 0].axhline(y=0.87, color='red', linestyle='--', label='Thesis Target (0.87)')
        axes[0, 0].axhline(y=0.86, color='orange', linestyle='--', label='Strong Performance (0.86)')
        axes[0, 0].set_ylabel('RÂ² Score')
        axes[0, 0].set_title('Enhanced Model Performance Comparison', fontsize=14, fontweight='bold')
        axes[0, 0].legend()
        axes[0, 0].grid(axis='y', alpha=0.3)

        # Add value labels
        for bar, score in zip(bars, r2_scores):
            axes[0, 0].text(bar.get_x() + bar.get_width()/2, score + 0.005,
                           f'{score:.4f}', ha='center', va='bottom', fontweight='bold')

        # 2. Enhanced Feature Importance (Top 10)
        top_features = feature_importance_df.head(10)
        axes[0, 1].barh(range(len(top_features)), top_features['gain'], color='lightgreen')
        axes[0, 1].set_yticks(range(len(top_features)))
        axes[0, 1].set_yticklabels(top_features.index)
        axes[0, 1].set_xlabel('Gain')
        axes[0, 1].set_title('Enhanced XGBoost Feature Importance (Top 10)', fontsize=14, fontweight='bold')
        axes[0, 1].grid(axis='x', alpha=0.3)

        # 3. Prediction vs Actual
        axes[0, 2].scatter(y_test, y_test_pred, alpha=0.6, color='purple')
        axes[0, 2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                       'r--', lw=2, label='Perfect Prediction')
        axes[0, 2].set_xlabel('Actual Charges ($)')
        axes[0, 2].set_ylabel('Predicted Charges ($)')
        axes[0, 2].set_title('Enhanced XGBoost: Predicted vs Actual', fontsize=14, fontweight='bold')
        axes[0, 2].legend()
        axes[0, 2].grid(alpha=0.3)

        # Add RÂ² annotation
        test_r2 = self.performance_metrics['test']['r2_score']
        axes[0, 2].text(0.05, 0.95, f'RÂ² = {test_r2:.4f}', transform=axes[0, 2].transAxes,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                       fontsize=12, fontweight='bold')

        # 4. Residuals Analysis
        axes[1, 0].scatter(y_test_pred, residuals, alpha=0.6, color='red')
        axes[1, 0].axhline(y=0, color='black', linestyle='--')
        axes[1, 0].set_xlabel('Predicted Charges ($)')
        axes[1, 0].set_ylabel('Residuals ($)')
        axes[1, 0].set_title('Enhanced XGBoost: Residuals Analysis', fontsize=14, fontweight='bold')
        axes[1, 0].grid(alpha=0.3)

        # 5. Feature Categories Impact
        original_features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']
        original_count = len([f for f in feature_importance_df.head(10).index if f in original_features])
        enhanced_count = 10 - original_count

        labels = ['Original\nFeatures', 'Enhanced\nFeatures']
        sizes = [original_count, enhanced_count]
        colors = ['lightblue', 'lightcoral']

        axes[1, 1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('Feature Impact in Top 10\n(Original vs Enhanced)', fontsize=14, fontweight='bold')

        # 6. Model Evolution Timeline
        # Show progression from enhanced preprocessing through models
        steps = ['Raw Data', 'Enhanced\nPreprocessing', 'Enhanced\nLinear Reg', 'Enhanced\nXGBoost']

        if self.baseline_comparison:
            scores = [0.8637, 0.8637, self.baseline_comparison['performance_metrics']['test']['r2_score'], test_r2]  # Assuming raw was around previous baseline
        else:
            scores = [0.8637, 0.8637, 0.8566, test_r2]  # Default values if baseline not available

        axes[1, 2].plot(steps, scores, marker='o', linewidth=2, markersize=8, color='darkgreen')
        axes[1, 2].axhline(y=0.87, color='red', linestyle='--', alpha=0.7, label='Thesis Target')
        axes[1, 2].set_ylabel('RÂ² Score')
        axes[1, 2].set_title('Model Evolution with Enhanced Data', fontsize=14, fontweight='bold')
        axes[1, 2].tick_params(axis='x', rotation=45)
        axes[1, 2].legend()
        axes[1, 2].grid(axis='y', alpha=0.3)

        # Add final score annotation
        axes[1, 2].text(len(steps)-1, test_r2 + 0.005, f'{test_r2:.4f}',
                       ha='center', va='bottom', fontweight='bold',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.tight_layout()
        plt.savefig('results/plots/03_enhanced_xgboost_baseline.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("âœ… Enhanced XGBoost visualization saved: results/plots/03_enhanced_xgboost_baseline.png")

    def save_enhanced_results(self, training_time):
        """Save enhanced XGBoost baseline results."""
        print("\n" + "=" * 50)
        print("SAVING ENHANCED XGBOOST RESULTS")
        print("=" * 50)

        # Save the model
        os.makedirs('results/models', exist_ok=True)
        model_path = 'results/models/enhanced_xgboost_baseline.pkl'
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        print(f"âœ… Enhanced XGBoost model saved: {model_path}")

        # Create comprehensive summary
        summary = {
            'model_type': 'Enhanced XGBoost Baseline',
            'enhancement_strategy': 'High-quality data preprocessing + strategic feature engineering',
            'data_quality_score': self.preprocessing_quality.get('quality_score', 'Unknown') if self.preprocessing_quality else 'Unknown',
            'features_used': len(self.feature_names),
            'feature_names': self.feature_names,
            'baseline_parameters': {
                'n_estimators': 200,
                'max_depth': 8,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'reg_lambda': 1,
                'min_child_weight': 1
            },
            'performance_metrics': self.performance_metrics,
            'baseline_comparison': self._create_baseline_comparison(),
            'thesis_target_achievement': {
                'target_r2': 0.87,
                'achieved_r2': self.performance_metrics['test']['r2_score'],
                'target_met': self.performance_metrics['test']['r2_score'] >= 0.87,
                'gap_to_target': max(0, 0.87 - self.performance_metrics['test']['r2_score'])
            },
            'overfitting_analysis': {
                'train_r2': self.performance_metrics['training']['r2_score'],
                'test_r2': self.performance_metrics['test']['r2_score'],
                'overfitting_gap': self.performance_metrics['training']['r2_score'] - self.performance_metrics['test']['r2_score']
            },
            'training_time_seconds': training_time,
            'next_steps': 'Hyperparameter optimization if target not achieved, then XAI implementation',
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # Save summary
        summary_path = 'results/models/enhanced_xgboost_baseline_summary.json'
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=self._json_serialize)
        print(f"âœ… Enhanced XGBoost summary saved: {summary_path}")

        # Print final summary
        self._print_final_summary(summary)

        return summary

    def _create_baseline_comparison(self):
        """Create baseline comparison data."""
        if self.baseline_comparison is None:
            return None

        linear_r2 = self.baseline_comparison['performance_metrics']['test']['r2_score']
        xgb_r2 = self.performance_metrics['test']['r2_score']

        return {
            'enhanced_linear_r2': linear_r2,
            'enhanced_xgboost_r2': xgb_r2,
            'improvement': xgb_r2 - linear_r2,
            'relative_improvement_percent': ((xgb_r2 - linear_r2) / linear_r2) * 100,
            'xgboost_superiority': xgb_r2 > linear_r2
        }

    def _json_serialize(self, obj):
        """Custom JSON serializer for numpy types."""
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj

    def _print_final_summary(self, summary):
        """Print comprehensive final summary."""
        print("\n" + "=" * 70)
        print("ENHANCED XGBOOST BASELINE - FINAL SUMMARY")
        print("=" * 70)

        test_r2 = summary['performance_metrics']['test']['r2_score']
        target_achievement = summary['thesis_target_achievement']

        print(f"âœ… Enhanced XGBoost baseline training completed successfully!")
        print(f"ğŸ“Š Final RÂ² Score: {test_r2:.4f}")
        print(f"ğŸ“ˆ Data Quality Score: {summary['data_quality_score']}/10.0")
        print(f"â±ï¸  Training Time: {summary['training_time_seconds']:.2f} seconds")

        print(f"\nğŸ¯ THESIS TARGET ACHIEVEMENT:")
        if target_achievement['target_met']:
            print(f"   ğŸ‰ THESIS TARGET ACHIEVED: RÂ² = {test_r2:.4f} â‰¥ 0.87")
            print(f"   ğŸ† Enhanced XGBoost exceeds thesis requirement!")
            print(f"   ğŸš€ Ready for XAI implementation (SHAP & LIME)!")
        else:
            gap = target_achievement['gap_to_target']
            print(f"   ğŸ“Š Current Performance: RÂ² = {test_r2:.4f}")
            print(f"   ğŸ¯ Gap to thesis target: {gap:.4f}")
            if gap < 0.01:
                print(f"   âœ… Very close to target - minor hyperparameter tuning should achieve it!")
            elif gap < 0.02:
                print(f"   âœ… Close to target - hyperparameter optimization should achieve it!")
            else:
                print(f"   ğŸ“ˆ Hyperparameter optimization recommended to achieve thesis target")

        # Baseline comparison
        if summary['baseline_comparison']:
            comp = summary['baseline_comparison']
            print(f"\nğŸ“ˆ Enhanced Model Comparison:")
            print(f"   Enhanced Linear Regression RÂ²: {comp['enhanced_linear_r2']:.4f}")
            print(f"   Enhanced XGBoost RÂ²: {comp['enhanced_xgboost_r2']:.4f}")
            print(f"   XGBoost improvement: {comp['improvement']:+.4f}")

            if comp['xgboost_superiority']:
                print(f"   âœ… XGBoost demonstrates superiority over Linear Regression!")
            else:
                print(f"   ğŸ“Š XGBoost performance comparable to Linear Regression")

        # Overfitting assessment
        overfitting = summary['overfitting_analysis']
        print(f"\nğŸ” Model Quality Assessment:")
        print(f"   Training RÂ²: {overfitting['train_r2']:.4f}")
        print(f"   Test RÂ²: {overfitting['test_r2']:.4f}")
        print(f"   Overfitting gap: {overfitting['overfitting_gap']:.4f}")

        if overfitting['overfitting_gap'] < 0.05:
            print(f"   âœ… Excellent generalization!")
        elif overfitting['overfitting_gap'] < 0.10:
            print(f"   âœ… Good generalization!")
        else:
            print(f"   âš ï¸  Some overfitting - consider regularization")

        print(f"\nğŸ”„ Next Steps:")
        if test_r2 >= 0.87:
            print(f"   ğŸ¯ READY FOR PHASE 4: Explainable AI (SHAP & LIME)")
            print(f"   ğŸ† Thesis target achieved - proceed with confidence!")
        else:
            print(f"   ğŸ”§ Hyperparameter optimization to achieve RÂ² > 0.87")
            print(f"   ğŸ¯ Then proceed to Phase 4: Explainable AI")

        print(f"   ğŸ“Š Dashboard development with enhanced model")
        print(f"   ğŸ“ Thesis documentation with strong results")


def main():
    """Main execution function for enhanced XGBoost baseline."""
    # Create results directories
    os.makedirs('results/plots', exist_ok=True)
    os.makedirs('results/models', exist_ok=True)

    # Initialize enhanced XGBoost
    xgb_enhanced = EnhancedXGBoostBaseline()

    # Load enhanced data and baseline comparison
    df, baseline_comparison = xgb_enhanced.load_enhanced_data_and_baseline()
    if df is None:
        return

    # Prepare features for XGBoost
    X, y, label_encoders = xgb_enhanced.prepare_features_for_xgboost(df)

    # Split data consistently with baseline
    X_train, X_val, X_test, y_train, y_val, y_test = xgb_enhanced.split_data_consistent(X, y)

    # Train enhanced XGBoost baseline
    training_time = xgb_enhanced.train_enhanced_xgboost_baseline(X_train, y_train, X_val, y_val)

    # Evaluate performance
    test_metrics = xgb_enhanced.evaluate_enhanced_performance(X_train, y_train, X_val, y_val, X_test, y_test)

    # Compare with enhanced baseline
    xgb_enhanced.compare_with_enhanced_baseline()

    # Analyze feature importance
    feature_importance = xgb_enhanced.analyze_enhanced_feature_importance()

    # Create visualizations
    xgb_enhanced.create_enhanced_visualizations(X_test, y_test, feature_importance)

    # Save results
    summary = xgb_enhanced.save_enhanced_results(training_time)

    print("\n" + "=" * 70)
    print("ENHANCED XGBOOST BASELINE COMPLETED")
    print("=" * 70)
    print("âœ… Enhanced XGBoost baseline successfully implemented")
    print("âœ… Performance evaluation and comparison completed")
    print("âœ… Feature importance analysis completed")
    print("âœ… Enhanced model ready for optimization or XAI")

    final_r2 = test_metrics['r2_score']
    if final_r2 >= 0.87:
        print(f"\nğŸ‰ THESIS SUCCESS: RÂ² = {final_r2:.4f} â‰¥ 0.87 achieved!")
        print(f"ğŸ† Enhanced XGBoost baseline exceeds thesis target!")
        print(f"ğŸš€ Ready for Phase 4: Explainable AI implementation!")
    else:
        gap = 0.87 - final_r2
        print(f"\nğŸ“Š Strong Performance: RÂ² = {final_r2:.4f}")
        print(f"ğŸ¯ Gap to thesis target: {gap:.4f}")
        if gap < 0.01:
            print(f"âœ… Very close - minor optimization should achieve target!")
        else:
            print(f"ğŸ”§ Hyperparameter optimization recommended to achieve RÂ² > 0.87")


if __name__ == "__main__":
    main()